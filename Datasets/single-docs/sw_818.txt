Textual difficulty

Textual difficulty means how easy or hard a text is to read. Research has shown that two main factors affect the ease with which texts are read.

A readability test is a way to measure a text for how easy it is to read. Readability tests give a prediction as to how difficult readers will find a particular text. They do this by measuring one or both of the two main causes, as follows:

Word difficulty is usually measured by vocabulary lists or word length. 
In 1923, Bertha A. Lively and Sidney L. Pressey published the first reading ease formula. They had been concerned that science textbooks in junior high school had so many technical words. They felt that teachers spent all class time explaining their meaning. They argued that their formula would help to measure and reduce the “vocabulary burden” of textbooks. Their formula used the Thorndike word list as a basis. Manually, it took three hours to apply the formula to a book.

Several vocabulary lists have been published by researchers. These lists are based on samples of published texts in English, and (less often) samples of recorded spoken language. The lists differ slightly according to the sources chosen, but they are very reliable. The items listed may represent more than one actual word; they are lemmas. For instance the entry "be" contains within it the occurrences of "is", "was", "be" and "are". The top 100 lemmas account for 50% of all the words in the Oxford English Corpus.

"The Reading Teachers Book of Lists" claims that the first 25 words make up about one-third of all printed material in English, and that the first 100 make up about one-half of all written material.

One of the first readability tests, the Dale–Chall formula, used a vocabulary list. It counted the number of listed words in a passage, and applied a formula which gave a grade level. It was used to rate textbooks for grade levels in US school districts.

It is easy, in principle, to use a vocabulary list as part of a computer-based readability measure. The list is organised as a look-up table. The percentage of listed words in a passage gives the data for the formula, and the user is presented with a grade level.

This is called an "index", or a "proxy". This is because word length is correlated with word frequency, and word frequency is correlated with word difficulty. "Longer words are, "on average", harder than short words". 

Word length is measured by counting the letters in each word, or by counting syllables. Since most syllables have one vowel, some computer programs count vowels per average word. A few tests measure the percentage of words on a list; the list is based on the known frequency of words in a language.

Sentence difficulty is usually measured by sentence length. This again is an index, because longer sentences are, on average, harder than short sentences. Computers count the number of words between full stops, but this is a second-best method. Humans can judge whether a semi-colon or colon should count as the end of a sentence for testing purposes.

Since both factors may vary independently of each other, the best prediction is gained by devising a formula with makes use of both indices. What this means is that a single score is produced for a text, and that score is looked up on a table or graph. That tells you how difficult the text is in terms of either a) an American school grade level, or b) an artificial scale of 0% to 100%. Either way is effective. What really makes a difference is:

It is possible to get a good prediction by getting a group of subjects to read through a passage, followed by multiple-choice questions. Even better is a method called cloze, where subjects fill in blanks on a text they have not seen before. The percentage of correctly completed blanks is an outstandingly good predictor of text difficulty.

Naturally, this kind of direct measure requires subjects and a skilled experimenter. It also requires the prior preparation of texts suitable for the chosen sample of subjects. The method is therefore too expensive for widespread use.

A person can perform readability tests himself by counting and doing some math, or by using word-processing software. 





Wikipedia Signposts 2015-06-24 surveyed recent studies of web information on medical topics, including articles in English wiki.

Their summary was:

That shows these articles, and presumably many other medical articles on English wiki, are written in prose far too difficult for the average member of the public. 




